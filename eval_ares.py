"""Evaluate the ARES optimization task 

Uses FODO-style approach 

task modes:
    - matched: Optimize the ARES lattice for a matched beam (no offsets).
    - mismatched: Optimize with beam offsets simulating misalignments.
    - matched_prior_newtask: Optimize with prior that knows the true offsets.
"""

import os
import numpy as np

import bo_cheetah_prior_ares as bo_ares
import cheetah
import pandas as pd
import torch
import tqdm
from xopt import VOCS, Evaluator, Xopt
from xopt.generators.bayesian import UpperConfidenceBoundGenerator
from xopt.generators.bayesian.models.standard import StandardModelConstructor
from xopt.generators.sequential.neldermead import NelderMeadGenerator


def main(args):
    # VOCS - 5 magnets
    vocs_config = """
        variables:
            q1: [-30, 30]
            q2: [-30, 30]
            cv: [-0.006, 0.006]
            q3: [-30, 30]
            ch: [-0.006, 0.006]
        objectives:
            mae: minimize
    """
    vocs = VOCS.from_yaml(vocs_config)

    # Evaluator setup based on task
    if args.task == "matched":
        # Default beam, no offsets
        incoming_beam = None
        evaluator = Evaluator(
            function=bo_ares.ares_problem_with_offsets,
            function_kwargs={"incoming_beam": incoming_beam},
        )
        print(f"\nüìã Task: matched (default beam, no offsets)")
        
    elif args.task == "mismatched":
        # Different beam characteristics
        incoming_beam = cheetah.ParameterBeam.from_parameters(
            mu_x=torch.tensor(8.2413e-07),
            mu_px=torch.tensor(5.9885e-08),
            mu_y=torch.tensor(-1.7276e-06),
            mu_py=torch.tensor(-1.1746e-07),
            sigma_x=torch.tensor(0.0002),
            sigma_px=torch.tensor(3.6794e-06),
            sigma_y=torch.tensor(0.0001),  # Changed from 0.0002
            sigma_py=torch.tensor(3.6941e-06),
            sigma_tau=torch.tensor(8.0116e-06),
            sigma_p=torch.tensor(0.0023),
            energy=torch.tensor(1.0732e+08),
            total_charge=torch.tensor(5.0e-13),
        )
        # Beam offsets simulating misalignments
        beam_offsets = {
            "q1": (0.0000, 0.0002),   # 0, 200 Œºm
            "q2": (0.0001, -0.0003),  # 100, -300 Œºm
            "q3": (-0.0001, 0.00015), # -100, 150 Œºm
        }
        evaluator = Evaluator(
            function=bo_ares.ares_problem_with_offsets,
            function_kwargs={
                "incoming_beam": incoming_beam,
                "beam_offsets": beam_offsets,
            },
        )
        print(f"\nüìã Task: mismatched")
        print(f"   Beam: sigma_y = 0.0001 (changed from 0.0002)")
        print(f"   Beam offsets: Q1=(0, 200Œºm), Q2=(100, -300Œºm), Q3=(-100, 150Œºm)")
        
    elif args.task == "matched_prior_newtask":
        incoming_beam = cheetah.ParameterBeam.from_parameters(
            mu_x=torch.tensor(8.2413e-07),
            mu_px=torch.tensor(5.9885e-08),
            mu_y=torch.tensor(-1.7276e-06),
            mu_py=torch.tensor(-1.1746e-07),
            sigma_x=torch.tensor(0.0002),
            sigma_px=torch.tensor(3.6794e-06),
            sigma_y=torch.tensor(0.0001),
            sigma_py=torch.tensor(3.6941e-06),
            sigma_tau=torch.tensor(8.0116e-06),
            sigma_p=torch.tensor(0.0023),
            energy=torch.tensor(1.0732e+08),
            total_charge=torch.tensor(5.0e-13),
        )
        beam_offsets = {
            "q1": (0.0000, 0.0002),
            "q2": (0.0001, -0.0003),
            "q3": (-0.0001, 0.00015),
        }
        evaluator = Evaluator(
            function=bo_ares.ares_problem_with_offsets,
            function_kwargs={
                "incoming_beam": incoming_beam,
                "beam_offsets": beam_offsets,
            },
        )
        print(f"\nüìã Task: matched_prior_newtask (prior knows true offsets)")
        
    else:
        raise ValueError(f"Invalid task: {args.task}")

    # Results storage
    df = pd.DataFrame()

    # Run multiple trials
    print(f"\nüî¨ Running {args.n_trials} trials with {args.max_evaluation_steps} steps each")
    print(f"   Optimizer: {args.optimizer}")
    
    for i in range(args.n_trials):
        print(f"\n{'='*70}")
        print(f"TRIAL {i+1}/{args.n_trials}")
        print(f"{'='*70}")

        # Initialize Generator
        if args.optimizer == "BO":
            generator = UpperConfidenceBoundGenerator(beta=2.0, vocs=vocs)
            print("Generator: BO with zero mean")
            
        elif args.optimizer == "BO_prior":
            if args.task == "matched":
                prior_mean_module = bo_ares.AresPriorMeanRevised()
                gp_constructor = StandardModelConstructor(
                    mean_modules={"mae": prior_mean_module}
                )
                print("Generator: BO with matched prior (not trainable)")
                
            elif args.task == "mismatched":
                prior_mean_module = bo_ares.AresPriorMeanRevised(
                    incoming_beam=incoming_beam
                )
                # Initialize with ZERO (wrong guess - will learn)
                prior_mean_module.q1_offset_x = 0.0
                prior_mean_module.q1_offset_y = 0.0
                prior_mean_module.q2_offset_x = 0.0
                prior_mean_module.q2_offset_y = 0.0
                prior_mean_module.q3_offset_x = 0.0
                prior_mean_module.q3_offset_y = 0.0
                
                gp_constructor = StandardModelConstructor(
                    mean_modules={"mae": prior_mean_module},
                    trainable_mean_keys=["mae"],  # Make it trainable!
                    use_low_noise_prior=False,
                )
                print("Generator: BO with mismatched prior (trainable)")
                print("  Initial offsets: all zeros (will learn true values)")
                
            elif args.task == "matched_prior_newtask":
                incoming_beam_prior = cheetah.ParameterBeam.from_parameters(
                    mu_x=torch.tensor(8.2413e-07),
                    mu_px=torch.tensor(5.9885e-08),
                    mu_y=torch.tensor(-1.7276e-06),
                    mu_py=torch.tensor(-1.1746e-07),
                    sigma_x=torch.tensor(0.0002),
                    sigma_px=torch.tensor(3.6794e-06),
                    sigma_y=torch.tensor(0.0001),
                    sigma_py=torch.tensor(3.6941e-06),
                    sigma_tau=torch.tensor(8.0116e-06),
                    sigma_p=torch.tensor(0.0023),
                    energy=torch.tensor(1.0732e+08),
                    total_charge=torch.tensor(5.0e-13),
                )
                prior_mean_module = bo_ares.AresPriorMeanRevised(
                    incoming_beam=incoming_beam_prior
                )
                # Initialize with CORRECT values
                prior_mean_module.q1_offset_x = 0.0000
                prior_mean_module.q1_offset_y = 0.0002
                prior_mean_module.q2_offset_x = 0.0001
                prior_mean_module.q2_offset_y = -0.0003
                prior_mean_module.q3_offset_x = -0.0001
                prior_mean_module.q3_offset_y = 0.00015
                
                gp_constructor = StandardModelConstructor(
                    mean_modules={"mae": prior_mean_module}
                    # NOT trainable (already correct)
                )
                print("Generator: BO with matched prior (correct values, not trainable)")
            
            generator = UpperConfidenceBoundGenerator(
                beta=2.0, vocs=vocs, gp_constructor=gp_constructor
            )
 
        elif args.optimizer == "NM":
            generator = NelderMeadGenerator(vocs=vocs)
            print("Generator: Nelder-Mead simplex")
            
        else:
            raise ValueError(f"Invalid optimizer: {args.optimizer}")

        # Initialize Xopt
        xopt = Xopt(
            vocs=vocs,
            evaluator=evaluator,
            generator=generator,
            max_evaluations=args.max_evaluation_steps,
        )
        
        # Fixed starting point
        initial_point = {
            "q1": 10.0,
            "q2": -10.0,
            "cv": 0.0,
            "q3": 10.0,
            "ch": 0.0,
        }
        
        print(f"\nüìç Initial point: q1={initial_point['q1']}, q2={initial_point['q2']}, ...")
        xopt.evaluate_data(initial_point)
        print(f"   Initial MAE: {xopt.data['mae'].iloc[0]:.6e}")

        # Optimization loop
        print(f"\nüîÑ Starting optimization...")
        
        for step_num in tqdm.tqdm(range(args.max_evaluation_steps), 
                                   desc=f"Trial {i+1}/{args.n_trials}"):
            xopt.step()

        # Post-processing
        xopt.data.index.name = "step"
        xopt.data["run"] = i
        xopt.data["best_mae"] = xopt.data["mae"].cummin()

        # ========== NEW: Save learned offsets (like FODO drift_length) ==========
        if args.optimizer == "BO_prior" and args.task in ["mismatched", "matched_prior_newtask"]:
            try:
                model = xopt.generator.model
                gp_model = model.models[0]
                learned_mean = gp_model.mean_module._model
                
                # Extract learned offset values
                xopt.data["learned_q1_offset_x"] = float(learned_mean.q1_offset_x.item())
                xopt.data["learned_q1_offset_y"] = float(learned_mean.q1_offset_y.item())
                xopt.data["learned_q2_offset_x"] = float(learned_mean.q2_offset_x.item())
                xopt.data["learned_q2_offset_y"] = float(learned_mean.q2_offset_y.item())
                xopt.data["learned_q3_offset_x"] = float(learned_mean.q3_offset_x.item())
                xopt.data["learned_q3_offset_y"] = float(learned_mean.q3_offset_y.item())
                
                print(f"\n  Final learned offsets (m):")
                print(f"    Q1: x={learned_mean.q1_offset_x.item():+.6f}, y={learned_mean.q1_offset_y.item():+.6f}")
                print(f"    Q2: x={learned_mean.q2_offset_x.item():+.6f}, y={learned_mean.q2_offset_y.item():+.6f}")
                print(f"    Q3: x={learned_mean.q3_offset_x.item():+.6f}, y={learned_mean.q3_offset_y.item():+.6f}")
                
                if args.task == "mismatched":
                    print(f"\n  Ground truth offsets (m):")
                    print(f"    Q1: x=+0.000000, y=+0.000200")
                    print(f"    Q2: x=+0.000100, y=-0.000300")
                    print(f"    Q3: x=-0.000100, y=+0.000150")
                    
                    # Calculate errors
                    errors = [
                        abs(learned_mean.q1_offset_x.item() - 0.000000),
                        abs(learned_mean.q1_offset_y.item() - 0.000200),
                        abs(learned_mean.q2_offset_x.item() - 0.000100),
                        abs(learned_mean.q2_offset_y.item() + 0.000300),
                        abs(learned_mean.q3_offset_x.item() + 0.000100),
                        abs(learned_mean.q3_offset_y.item() - 0.000150),
                    ]
                    avg_error = sum(errors) / len(errors)
                    print(f"\n  Average learning error: {avg_error*1e6:.1f} Œºm")
                    
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Could not save learned offsets: {e}")
                import traceback
                traceback.print_exc()
        # ========== END NEW CODE ==========
        
        # Convert to float
        for col in xopt.data.columns:
            xopt.data[col] = xopt.data[col].astype(float)
        
        df = pd.concat([df, xopt.data])
        
        # Trial summary
        print(f"\n{'='*70}")
        print(f"TRIAL {i+1} COMPLETE")
        print(f"{'='*70}")
        print(f"Best MAE: {xopt.data['mae'].min():.6e}")

    # Save results
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    
    out_filename = f"{args.output_dir}/{args.optimizer}_{args.task}_revised.csv"
    df.to_csv(out_filename)
    
    print(f"\n{'='*70}")
    print(f"‚úÖ ALL TRIALS COMPLETE")
    print(f"{'='*70}")
    print(f"Results saved to: {out_filename}")
    print(f"Total evaluations: {len(df)}")
    print(f"Best MAE found: {df['mae'].min():.6e}")
    
    # Summary statistics
    final_bests = df.groupby('run')['best_mae'].last()
    print(f"\nFinal Results Across {args.n_trials} Trials:")
    print(f"  Mean:   {final_bests.mean():.6e}")
    print(f"  Std:    {final_bests.std():.6e}")
    print(f"  Median: {final_bests.median():.6e}")
    print(f"  Min:    {final_bests.min():.6e}")
    print(f"  Max:    {final_bests.max():.6e}")
    
    # ========== NEW: Offset learning summary ==========
    if args.optimizer == "BO_prior" and "learned_q1_offset_y" in df.columns:
        print(f"\nLearned Offset Summary:")
        final_offsets = df.groupby('run')['learned_q1_offset_y'].last()
        print(f"  Q1 Y offset: {final_offsets.mean():.6f} ¬± {final_offsets.std():.6f} m")
        
        if args.task == "mismatched":
            print(f"  Ground truth: 0.000200 m")
            print(f"  Mean error: {abs(final_offsets.mean() - 0.0002)*1e6:.1f} Œºm")
    # ========== END NEW CODE ==========


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Run ARES optimization task - REVISED with FODO-style offsets."
    )
    parser.add_argument(
        "--optimizer",
        type=str,
        default="BO",
        choices=["BO", "BO_prior", "NM"],
        help="Optimizer to use",
    )
    parser.add_argument(
        "--n_trials",
        "-n",
        type=int,
        default=5,
        help="Number of trials to run",
    )
    parser.add_argument(
        "--task",
        "-t",
        type=str,
        default="matched",
        choices=["matched", "mismatched", "matched_prior_newtask"],
        help="Task to run",
    )
    parser.add_argument(
        "--max_evaluation_steps",
        "-s",
        type=int,
        default=130,
        help="Maximum number of evaluations per trial",
    )
    parser.add_argument(
        "--output_dir",
        "-o",
        type=str,
        default="data/",
        help="Output directory for results",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility",
    )
    
    args = parser.parse_args()
    
    # Set random seed
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    main(args)