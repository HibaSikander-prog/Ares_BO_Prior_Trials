{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "\n",
    "# Plotting Styles\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "plt.style.use([\"science\", \"ieee\", \"no-latex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load results from all experiments\n",
    "df_nm_mismatched = pd.read_csv(\"data/NM_mismatched.csv\")\n",
    "df_bo_mismatched = pd.read_csv(\"data/BO_mismatched.csv\")\n",
    "df_bo_prior_mismatched = pd.read_csv(\"data/BO_prior_mismatched.csv\")\n",
    "df_bo_prior_matched_newtask = pd.read_csv(\"data/BO_prior_matched_prior_newtask.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Find best MAE achieved across all methods\n",
    "all_dfs = [df_nm_mismatched, df_bo_mismatched, df_bo_prior_mismatched, df_bo_prior_matched_newtask]\n",
    "\n",
    "global_best_mae = min([df['mae'].min() for df in all_dfs])\n",
    "\n",
    "print(f\"Best MAE found across all methods: {global_best_mae:.6e}\")\n",
    "print(f\"Best MAE (mm): {global_best_mae * 1000:.4f}\")\n",
    "\n",
    "# Find which method achieved it\n",
    "method_names = ['NM', 'BO', 'BO_prior (mismatched)', 'BO_prior (matched)']\n",
    "for name, df in zip(method_names, all_dfs):\n",
    "    if df['mae'].min() == global_best_mae: \n",
    "        print(f\"Achieved by: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "figsize = list(plt.rcParams[\"figure.figsize\"])\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = plt.gca()\n",
    "\n",
    "# Plot convergence curves\n",
    "sns.lineplot(\n",
    "    data=df_nm_mismatched,\n",
    "    x=\"step\",\n",
    "    y=\"best_mae\",\n",
    "    ax=ax,\n",
    "    label=\"Simplex (NM)\",\n",
    "    color=colors[0],\n",
    "    ls=\"-\",\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_bo_mismatched,\n",
    "    x=\"step\",\n",
    "    y=\"best_mae\",\n",
    "    ax=ax,\n",
    "    label=\"BO (zero mean)\",\n",
    "    color=colors[1],\n",
    "    ls=\"-.\",\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_bo_prior_mismatched,\n",
    "    x=\"step\",\n",
    "    y=\"best_mae\",\n",
    "    ax=ax,\n",
    "    label=\"BO mismatched prior (trainable)\",\n",
    "    color=colors[2],\n",
    "    ls=\"--\",\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_bo_prior_matched_newtask,\n",
    "\n",
    "    x=\"step\",\n",
    "    y=\"best_mae\",\n",
    "    ax=ax,\n",
    "    label=\"BO matched prior\",\n",
    "    color=colors[3],\n",
    "    ls=\":\",\n",
    ")\n",
    "\n",
    "# Add reference line for best found\n",
    "ax.axhline(y=global_best_mae, color=\"grey\", linestyle=\"--\", alpha=0.5, label=\"Best found\")\n",
    "\n",
    "# Format y-axis to show mm\n",
    "yticks = ax.get_yticks()\n",
    "ax.set_yticklabels([f\"{y*1000:.2f}\" for y in yticks])\n",
    "ax.set_ylabel(\"Beam size (mm)\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_title(\"ARES Lattice Optimization: Convergence Comparison\")\n",
    "ax.set_xlim(0, 150)\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"ares_bo_prior_result.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate statistics for each method\n",
    "def compute_stats(df, name):\n",
    "    final_best = df.groupby('run')['best_mae'].last()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean final best MAE:  {final_best.mean():.6e} ({final_best.mean()*1000:.4f} mm)\")\n",
    "    print(f\"  Std final best MAE: {final_best.std():.6e} ({final_best.std()*1000:.4f} mm)\")\n",
    "    print(f\"  Min final best MAE: {final_best.min():.6e} ({final_best.min()*1000:.4f} mm)\")\n",
    "    print(f\"  Max final best MAE: {final_best.max():.6e} ({final_best.max()*1000:.4f} mm)\")\n",
    "    \n",
    "    # Count how many runs reached within 10% of global best\n",
    "    threshold = global_best_mae * 1.1\n",
    "    success_rate = (final_best <= threshold).sum() / len(final_best) * 100\n",
    "    print(f\"  Success rate (within 10% of best): {success_rate:.1f}%\")\n",
    "\n",
    "compute_stats(df_nm_mismatched, \"Nelder-Mead\")\n",
    "compute_stats(df_bo_mismatched, \"BO (zero mean)\")\n",
    "compute_stats(df_bo_prior_mismatched, \"BO mismatched prior\")\n",
    "compute_stats(df_bo_prior_matched_newtask, \"BO matched prior\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate how many iterations to reach certain thresholds\n",
    "threshold = global_best_mae * 1.2  # 20% above best\n",
    "\n",
    "def iterations_to_threshold(df, threshold):\n",
    "    \"\"\"Calculate iterations needed to reach threshold for each run\"\"\"\n",
    "    iterations = []\n",
    "    for run_id in df['run'].unique():\n",
    "        run_data = df[df['run'] == run_id]\n",
    "        below_threshold = run_data[run_data['best_mae'] <= threshold]\n",
    "        if len(below_threshold) > 0:\n",
    "            iterations.append(below_threshold.index[0] - run_data.index[0])\n",
    "        else:\n",
    "            iterations.append(np.nan)  # Never reached\n",
    "    return iterations\n",
    "\n",
    "print(f\"\\nIterations to reach {threshold*1000:.4f} mm:\")\n",
    "for name, df in zip(method_names, all_dfs):\n",
    "    iters = iterations_to_threshold(df, threshold)\n",
    "    valid_iters = [i for i in iters if not np.isnan(i)]\n",
    "    if valid_iters:\n",
    "        print(f\"{name}: {np.mean(valid_iters):.1f} Â± {np.std(valid_iters):.1f} iterations ({len(valid_iters)}/{len(iters)} runs)\")\n",
    "    else:\n",
    "        print(f\"{name}: Never reached threshold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
